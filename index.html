<html>
<head>
<title> NeurIPS 2020 Demo </title>
</head>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">  
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tone/13.8.21/Tone.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0/es6/core.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0"></script>
<style>
.collapsible {
  background-color: #777;
  color: white;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: center;
  outline: none;
  font-size: 15px;
  height: 60px;
}

.active, .collapsible:hover {
  background-color: #555;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#span {
  background-color: LightBlue;
}

#hr_top{
    border-color:black;
    border-width: 4px;
    width: 50%;
}

#hr_bottom{
    border-color:black;
    border-width: 1px;
    width: 50%;
}
</style>

<body>
<div>
    <hr id="hr_top" style="width:55%;">
    <h1 style="text-align: center;">Musical Speech</h1>
    <h3 style="text-align: center;"> A Transformer-based Composition Tool</h3>
    <hr id="hr_bottom" style="width:55%;">
</div>

<center>

    <b>Jason d'Eon<sup>1,2,*</sup>, Sri Harsha Dumpala<sup>1,2,*</sup>, Chandramouli Sastry<sup>1,2,*</sup>, Daniel Oore<sup>3</sup>, Mengyu Yang<sup>4</sup>, Sageev Oore<sup>1,2</sup> </b>
    <br><p style="font-size:1.75vmin;">
    <sup>1</sup>Dalhousie University, <sup>2</sup>Vector Institute, <sup>3</sup>IICSI, Memorial University of Newfoundland, <sup>4</sup>University of Toronto<br>
  <sup>*</sup> equal contribution<br>
  {jason.n.deon,chandramouli.sastry,osageev} at gmail.
    </p>
   
</center>

<center>

    <button type="button" class="collapsible" style="width:55%;"><b>Abstract</b></button>
    <div class="content" style="width:55%; background-color: WhiteSmoke">
        <br>
        <p style="text-align: justify; font-size:1.75vmin;">
            In this demo we propose a compositional tool that generates musical sequences based on prosody of speech recorded by the user.
            The tool allows any user–regardless of musical training-to use their own speech to generate musical melodies, while hearing the direct connection between their recorded speech and resulting music.
            This is achieved with a pipeline combining speech-based signal processing, musical heuristics, and a set of transformer models trained for new musical tasks.
            Importantly, the pipeline is designed to work with any kind of speech input and does not require a paired dataset for the training of the said transformer model.
            Our approach consists of the following steps:
        </p>
        <ol style="text-align: justify; font-size:1.75vmin;">
            <li>Estimate the F0 values and loudness envelope of the speech signal.</li>
            <li>Convert this into a sequence of musical constraints derived from the speech signal.</li>
            <li>Apply one or more transformer models—each trained on different musical tasks or datasets—to this constraint sequence to produce musical sequences that follow or accompany the speech patterns in a variety of ways.</li>
        </ol>
        <p style="text-align: justify; font-size:1.75vmin;">
            The demo is self-explanatory: a user can interact with the system by either providing a live-recording using a web-based recording interface or by uploading a pre-recorded speech sample.
            The system then provides a visualization of the F0 contour extracted from the provided speech sample, the set of note constraints obtained from the speech, and the sequence of musical notes as generated by the transformers.
            The user can also listen to—and interactively mix the levels (volume) of—the input speech sample, initial note sequences, and the musical sequences as generated by the transformer models.
        </p>
    </div>
    
    <br></br>
    
    <div style="width:90%;">
      <a href="https://panther.research.cs.dal.ca/demo">Try out</a> our interactive demo! <br>
    </div>
  
    <div style="width:90%;">
      <a href="https://jasondeon.github.io/musicalSpeech/samples.html">Listen to musical samples</a> created using this system.
  </div>
    
    <br></br>
    
    <!-- <div style="width:35%; display:inline-block;"> -->
    <div style="width:35%; display:inline-block">
        <span><b>&nbsp;What is musical speech?&nbsp;</b></span><br>
        <iframe width="420" height="345" style="border-style:inset" allowfullscreen src="https://www.youtube.com/embed/IjTnt_MP86M"></iframe>
    </div>
    <div style="width:35%; display:inline-block;">
        <span><b>&nbsp;The video demo will be posted here soon!&nbsp;</b></span><br>
        <iframe width="420" height="345" style="border-style:inset" allowfullscreen></iframe>
    </div>
    
</center>

<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

</body>
</html>
