<html>
<head>
<title> NeurIPS 2020 Musical Speech FAQ </title>
</head>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">  
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tone/13.8.21/Tone.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0/es6/core.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0"></script>

<style>
.collapsible {
  background-color: #777;
  color: white;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: center;
  outline: none;
  font-size: 15px;
  height: 55px;
}
.simple_border {
  background-color: #777;
  color: white;
/*   cursor: pointer; */
  padding: 18px;
  width: 100%;
  border: none;
  text-align: center;
  outline: none;
  font-size: 15px;
  height: 60px;
}
.active, .collapsible:hover {
  background-color: #B0C4DE;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}
.divider{
    width: 100%;
    height: 5px;
    display: inline-block;
}

#span {
  background-color: LightBlue;
}

#hr_top{
    border-color:black;
    border-width: 4px;
    width: 50%;
}

#hr_bottom{
    border-color:black;
    border-width: 1px;
    width: 50%;
}
  
div{
  text-align: justify;
}
</style>


<body>
<div>
    <hr id="hr_top" style="width:55%;">
    <h1 style="text-align: center;">Musical Speech</h1>
    <h3 style="text-align: center;"> A Transformer-based Composition Tool</h3>
    <hr id="hr_bottom" style="width:55%;">
</div>

<center>

    <div style="width:55%">
    <h5 style="text-align: center;"> F.A.Q. (Frequently Asked Questions)</h5>
    </div>
<br><br>
  
    <button type="button" class="collapsible" style="width:55%; background-color:#DCDCDC; color: black;"><b>What is the purpose of this tool?</b></button>
  <div class="content" style="width:75%; background-color: WhiteSmoke">
      <br>
      <p style="text-align: justify; font-size:1.75vmin;">

This system is meant to be a tool to assist in musical composition by
analysing speech and generating potential musical material based on that
speech. While we find this material interesting to listen to on its own,
and often draws our attention to elements of the speech that we might not
have otherwise noticed, this material is in no way intended to be a complete
(i.e. "stand-alone") composition in itself.
Rather, it is material which can then be taken by a composer or producer
and used for further processing!

See the answers to "Has anything like this been done before?" to better
understand the underlying motivation, and see the answers to
"How were the samples created?" to better understand how this material
can be processed.
        
A simplified version of the potential workflow is illustrated in this sample demo, which illustrates the progression, beginning with (1) raw speech, followed by 
        (2) close pitch tracking, (3) transformer output with slight adjustment, (4) accompaniment created to accompany the output of the previous step
        and finally (5) mixing it all together. 
    <div style="width:55%;padding:5px">
  <p><b>Fun to have fun</b></p>
    <audio controls>
    <source src="workshop_samples/funToHaveFunExpo.wav" type="audio/wav">
    Your browser does not support the audio element.
    </audio><br></br>
  </div>
    </p>
  </div>
  
  <div class="divider">
  </div>
  
      <button type="button" class="collapsible" style="width:55%; background-color:#808080; color: white;"><b>Has anything like this been done before?</b></button>
  <div class="content" style="width:75%; background-color: WhiteSmoke";">
      <br>
<p style="text-align: justify; font-size:1.75vmin;">
Absolutely! This is very much inspired by some of the favourite techniques of some of the authors! (Though it has almost always involved an extensive 
manual element, with few supporting technologies other than rewind-and-listen-again.) Just a few examples include:
<ul>
<li>The great Brazilian jazz pianist and composer, Hermeto Pascoal, used to do this often, and referred to it as the "Aura Sound" of a person's voice. 
You can watch him do this with the voice of actor Yves Montand in
<a href="https://www.youtube.com/watch?v=SrgveUpwCnM">this video</a>, where he first finds the pitches and then improvises music around these pitches.
(While he might make it look easy, it takes a lot of practise to be able to
  hear and interpret the musical pitch of a speaking voice!)</li>
<li> There have been entire albums, in a variety of genres, that have been inspired
by the explicit conversion of speech into music, and much more work where the musicality
of speech is an implicit source of inspiration. For example, <a href="https://www.youtube.com/watch?v=gRmtvGk5IHw">this movie</a> 
  is about an album called the Happiness Project, based on interviews with people in Toronto.</li>
<li> In 2009, a ``speaking piano'' recited the Proclamation of the European Environmental Criminal Court at the World Venice Forum. 
This was the work of composer Peter Ablinger, and narrator Miro Markus (an elementary school student at the time).</li>
<li>One of the co-authors has been using this approach in his own musical compositions (e.g.
  <a href="https://soundcloud.com/dani-oore/marianne-marianne-rough-mix1">Marianne</a>,
<a href="https://www.youtube.com/watch?v=3xMKQEexFZM">Riperian Dan</a>,
<a href="https://www.youtube.com/watch?v=vxdIwyYw_b4&feature=emb_logo">Sound is Touch</a>).</li>
<li>The application of this technique specifically to speeches by Donald Trump is discussed and analyzed (along with a youtube playlist) in 
  <a href="http://dani.oore.ca/trump/">this book chapter</a> by D. Oore</li> 
  <!--Daniel Oore, TRUMP THE MUSICAL PROPHET, in You Shook Me All Campaign Long: Music in the 2016 Presidential Election and Beyond, ed. Eric T. Kasper and Benjamin S. Schoening (Denton: University of North Texas Press, 2018).-->
  </li>
    <li>Some commercial music production software (e.g. Ableton Live) has begun to try to incorporate some proprietary voice-pitch-tracking technology.</li>
<li>Another co-author had to use the "inverse" (i.e. generative) process to control the prosody of speech in real-time: 
in the <a href="https://www.youtube.com/watch?v=Mb7K2IpZn6E">GloveTalk II</a> project, he used his vertical hand position to control the 
F0 value of a speech synthesizer. This required effectively "hearing" the musical pitch contour in a way disentangled 
from the rest of the speech signal.</li>
    </ul>
  Despite all these examples, the process of going from a raw speech recording to a produced musical excerpt is one that requires extensive fine-tuning. To some extent,
  this will always be the case, but our system aims to support this process, and uses generative musical models to do so, thus providing the composer and producer with controllable options to allow 
  transparent and easier iteration.
  </p>
  </div>
  
  <div class="divider">
  </div>
    
      <button type="button" class="collapsible" style="width:55%;  background-color:#DCDCDC; color: black;"><b>How were the samples created?</b></button>
  <div class="content" style="width:75%; background-color: WhiteSmoke">
      <br>
 <p style="text-align: justify; font-size:1.75vmin;">
To be clear, we assume this question is referring not to the saved examples on the interactive page, but to samples such as this one:
         <div style="width:55%;padding:5px">
  <br><b>Moo cow</b></br>
    <audio controls>
    <source src="workshop_samples/moo cow.wav" type="audio/wav">
    Your browser does not support the audio element.
    </audio><br></br>
  </div>
and others on 
<a href="https://jasondeon.github.io/musicalSpeech/samples.html">this page</a>.

This question is answered <a href="http://dani.oore.ca/moocow/">here</a> in some detail by one of the co-authors, composer
Dani Oore.
  
  </p>
</div>

  <div class="divider">
  </div>
  
  <button type="button" class="collapsible" style="width:55%; background-color:#808080; color: white;"><b>What features are extracted from speech?</b></button>
  <div class="content" style="width:75%; background-color: WhiteSmoke">
      <br>
      <p style="text-align: justify; font-size:1.75vmin;">
        Fundamental frequncy (F0) and loudness envelope of the speech signal are computed.
        The features are extracted at frame-level using a frame size of 50 msec and frame shift of 20 msec.
      </p>
  </div>

  <div class="divider">
  </div>
  
    <button type="button" class="collapsible" style="width:55%;background-color:#DCDCDC; color: black;"><b>What is meant by sparsification?</b></button>
  <div class="content" style="width:75%; background-color: WhiteSmoke">
      <br>
      <p style="text-align: justify; font-size:1.75vmin;">
        Loudness envelope extracted from the speech signal is processed to obtain regions of interest in speech.
        The regions of interest are selected using two different approaches:
       </p>
       <ol style="text-align: justify; font-size:1.75vmin;">
            <li>Approach 1 - Pre-defined threshold: In this approach, only frames with loudness value above a pre-defined threshold are potential regions for further processing.
              The pre-defined threshold is set based on empirical analysis.</li>
            <li>Approach 2 - Peak picking: In this approach, peaks in the loudness envelope are considered as the potential regions for further processing.
                The peaks in the loudness envelope generally fall in the vicinity of syllable nuclei.
        </ol>
        The selected regions are further refined by selecting only regions where F0 values are greater than 60 Hz. This step is performed to ensure that the regions of 
        interest fall in the voiced regions where the F0 value estimation is more reliable. We will consider the F0 values obtained in the retained frames for further processing. 
        <b></b>                                                
        In our <a href="https://panther.research.cs.dal.ca/demo">interactive demo</a>, in the sparsification type, v1 (v1:Low, v1:Medium and v1:High) follows 
        Approach 1 and v2 (v2:Low, v2:Medium and v2:High) follows Approach 2, respectively, to obtain the sparsified representation of the speech.
        

      </p>
  </div>

  <div class="divider">
  </div>
  
    <button type="button" class="collapsible" style="width:55%;background-color:#808080; color: white;"><b>What are the different levels in sparsification?</b></button>
  <div class="content" style="width:75%; background-color: WhiteSmoke">
      <br>
      <p style="text-align: justify; font-size:1.75vmin;">
        In our <a href="https://panther.research.cs.dal.ca/demo">interactive demo</a>, under sparsification type, you can see 6 options (v1:Low, v2:Low, v1:Medium, v2:Medium, v1:High and v2:High).
        The different options with v1 (v1:Low, v1:Medium and v1:High) and v2 (v2:Low, v2:Medium and v2:High) refer to the different levels of sparsification 
        obtained using Approach 1 and Approach 2 (explained in above question), respectively.
        The options Low, Medium, High refer to the different levels of sparsification. Here "Low" refers to lower level of sparsification (most of the F0 values extracted from speech are retained) and "High"
        refers to the higher level of sparsification (very few of the F0 values extracted from speech are retained).
        <br></br>
        For Approach 1, the threshold is varied depending on the level of sparsification. For Approach 2, the context (number of frames selected) around each peak location are varied depending on the level of sparsification.
        For instance in Approach 2, for the option v2:Low (lower level of sparsification), two frames on each side of the peak location (along with the peak location) are selected for further processing.
      </p>
  </div>

  <div class="divider">
  </div>
                      
  
    <button type="button" class="collapsible" style="width:55%; background-color:#DCDCDC; color: black;"><b>Can we try it for longer examples?</b></button>
  <div class="content" style="width:75%; background-color: WhiteSmoke">
      <br>
      <p style="text-align: justify; font-size:1.75vmin;">
Please feel free to get in touch with us, we'd love to hear from you!
    </p>
  </div>

  <div class="divider">
  </div>
  
</center>

<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>


</body>
</html>
